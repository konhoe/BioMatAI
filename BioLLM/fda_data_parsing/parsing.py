# parsing.py (ìˆ˜ì • ë²„ì „)
import argparse
import json
import random
import time
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, urlencode
import re

import requests
import fitz  # PyMuPDF

from selenium import webdriver
from selenium.common.exceptions import (
    TimeoutException,
    NoSuchElementException,
    StaleElementReferenceException,
    WebDriverException,
)
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager

BASE_URL = "https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfPMN/pmn.cfm"

# ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ (ìƒì„¸ í˜ì´ì§€ì—ì„œ PDF ì„ íƒ ê¸°ì¤€)
SUMMARY_KEYWORDS = [
    "summary",
    "decision summary",
    "summary (english)",
]
BACKUP_PDF_KEYWORDS = [
    "decision letter",
    "substantial equivalence",
    "se letter",
    "clearance",
    "letter",
    "decision",
    "determination",
]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Selenium setup
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def setup_driver(headless: bool = True) -> webdriver.Chrome:  # ê¸°ë³¸ê°’ì„ Trueë¡œ ë³€ê²½
    options = webdriver.ChromeOptions()
    if headless:
        # new headlessê°€ ì•ˆì •ì 
        options.add_argument("--headless=new")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.set_page_load_timeout(60)
    return driver


def requests_session_from_driver(driver: webdriver.Chrome) -> requests.Session:
    s = requests.Session()
    s.headers.update(
        {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,application/pdf;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.8",
            "Connection": "keep-alive",
        }
    )
    for c in driver.get_cookies():
        try:
            s.cookies.set(c["name"], c["value"], domain=c.get("domain"), path=c.get("path", "/"))
        except Exception:
            s.cookies.set(c["name"], c["value"])
    return s


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rand_sleep(a=1.0, b=2.0):
    time.sleep(random.uniform(a, b))


def extract_pdf_text_with_session(
    session: requests.Session, pdf_url: str, referer: Optional[str] = None, tries: int = 3
) -> Optional[str]:
    backoff = 1.2
    for attempt in range(1, tries + 1):
        try:
            if referer:
                session.headers.update({"Referer": referer})

            r = session.get(pdf_url, allow_redirects=True, timeout=90)
            final_url = r.url

            # FDA ì°¨ë‹¨ í˜ì´ì§€ íŒ¨í„´
            if "apology_objects" in final_url.lower():
                print(f"[apology ì°¨ë‹¨] {final_url}")
                return None

            ctype = (r.headers.get("Content-Type") or "").lower()
            dispo = (r.headers.get("Content-Disposition") or "").lower()

            # PDF íŒì •: (1) URL í™•ì¥ì (2) content-type (3) content-disposition filename
            is_pdf = (
                final_url.lower().endswith(".pdf")
                or "application/pdf" in ctype
                or ".pdf" in dispo
            )
            if not is_pdf:
                print(f"[PDF ì•„ë‹˜] {final_url} (Content-Type={ctype})")
                return None

            with fitz.open(stream=r.content, filetype="pdf") as doc:
                text_parts = []
                for p in doc:
                    text_parts.append(p.get_text("text"))
            plain = "\n".join(text_parts).strip()
            if not plain:
                print(f"[í…ìŠ¤íŠ¸ ì—†ìŒ] {final_url}")
                return None

            rand_sleep(1.0, 1.8)
            return plain
        except Exception as e:
            if attempt >= tries:
                print(f"[PDF ì¶”ì¶œ ì‹¤íŒ¨ {attempt}/{tries}] {pdf_url}: {e}")
                return None
            # ì§€ìˆ˜ ë°±ì˜¤í”„
            sleep_s = backoff ** attempt
            time.sleep(sleep_s)
            continue


def wait_for_search_box(driver: webdriver.Chrome) -> None:
    WebDriverWait(driver, 20).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "input[name='DeviceName']"))
    )


def safe_find_all_results_rows(driver: webdriver.Chrome):
    # ê²°ê³¼ í–‰: ì²« ì»¬ëŸ¼ì´ ìƒì„¸ë§í¬( ?ID= )ë¥¼ í¬í•¨í•˜ëŠ” tr
    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, "//table")))
    rows = driver.find_elements(By.XPATH, "//table//tr[td/a[contains(@href,'pmn.cfm?ID=')]]")
    return rows


def debug_current_page_info(driver: webdriver.Chrome):
    """í˜„ì¬ í˜ì´ì§€ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¶œë ¥"""
    try:
        print(f"    â†’ í˜„ì¬ URL: {driver.current_url}")
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ì˜ì—­ í™•ì¸
        pagination_links = driver.find_elements(By.XPATH, "//a[contains(@href, 'PAGENUM=')]")
        print(f"    â†’ í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ ê°œìˆ˜: {len(pagination_links)}")
        
        if pagination_links:
            for i, link in enumerate(pagination_links[:10]):  # ì²˜ìŒ 10ê°œë§Œ
                href = link.get_attribute("href")
                text = link.text.strip()
                print(f"      - ë§í¬ {i+1}: '{text}' -> {href}")
        
        # '>' ë²„íŠ¼ í™•ì¸
        next_arrows = driver.find_elements(By.XPATH, "//a[text()='>']")
        print(f"    â†’ '>' ë²„íŠ¼ ê°œìˆ˜: {len(next_arrows)}")
        if next_arrows:
            href = next_arrows[0].get_attribute("href")
            print(f"      - '>' ë§í¬: {href}")
        
        # ê²°ê³¼ ê°œìˆ˜ì™€ í˜ì´ì§€ ì •ë³´ í…ìŠ¤íŠ¸ í™•ì¸
        body_text = driver.find_element(By.TAG_NAME, "body").text
        if "to" in body_text and "of" in body_text:
            # "1 to 10 of 500 Results" ê°™ì€ íŒ¨í„´ ì°¾ê¸°
            import re
            match = re.search(r'(\d+)\s+to\s+(\d+)\s+of\s+(\d+)', body_text)
            if match:
                start, end, total = match.groups()
                print(f"    â†’ ê²°ê³¼ ë²”ìœ„: {start}-{end} of {total}")
        
    except Exception as e:
        print(f"    â†’ ë””ë²„ê·¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")


def first_row_signature(driver) -> str:
    """ê²°ê³¼ í…Œì´ë¸” ì²« í–‰ì˜ ê³ ìœ  ì„œëª…(K-numberì™€ device name ì¡°í•©)"""
    try:
        rows = safe_find_all_results_rows(driver)
        if not rows:
            return ""
        
        # ì²« ë²ˆì§¸ í–‰ì˜ K-numberì™€ device nameì„ ì¡°í•©í•˜ì—¬ ê³ ìœ  ì‹œê·¸ë‹ˆì²˜ ìƒì„±
        first_row = rows[0]
        k_number = first_row.find_element(By.XPATH, ".//td[1]/a").text.strip()
        device_name = first_row.find_element(By.XPATH, ".//td[2]").text.strip()[:50]  # ì²˜ìŒ 50ìë§Œ
        
        signature = f"{k_number}||{device_name}"
        print(f"    â†’ ì²« í–‰ ì‹œê·¸ë‹ˆì²˜: {signature}")
        return signature
    except Exception as e:
        print(f"    â†’ ì²« í–‰ ì‹œê·¸ë‹ˆì²˜ ìƒì„± ì‹¤íŒ¨: {e}")
        return ""


def count_total_results(driver) -> int:
    """í˜„ì¬ í˜ì´ì§€ì˜ ê²°ê³¼ ê°œìˆ˜ë¥¼ ë°˜í™˜"""
    try:
        rows = safe_find_all_results_rows(driver)
        return len(rows)
    except Exception:
        return 0


# URL helpers for robust next-page handling
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

def _abs(driver, href: str) -> str:
    return urljoin(driver.current_url, href)

def _bump_pagenum_in_href(href: str, step: int = 10) -> Optional[str]:
    try:
        u = urlparse(href)
        q = parse_qs(u.query)
        # FDAëŠ” PAGENUM ë˜ëŠ” start íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©
        if "PAGENUM" in q:
            cur = int(q["PAGENUM"][0])
            q["PAGENUM"] = [str(cur + step)]
        elif "start" in q:
            cur = int(q["start"][0])
            q["start"] = [str(cur + step)]
        else:
            # ì—†ëŠ” ê²½ìš°ì—” ìƒˆë¡œ ì¶”ê°€ (1â†’11ë¡œ ê°€ì •)
            q["PAGENUM"] = ["11"]
        new_q = urlencode({k: v[0] for k, v in q.items()})
        return urlunparse((u.scheme, u.netloc, u.path, u.params, new_q, u.fragment))
    except Exception:
        return None


# FDA ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ URLì„ ì§ì ‘ êµ¬ì„± (GET íŒŒë¼ë¯¸í„° ë°©ì‹)
def find_next_page_element(driver: webdriver.Chrome):
    """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•  ìˆ˜ ìˆëŠ” ìš”ì†Œë¥¼ ì°¾ìŒ (FDA ìŠ¤íƒ€ì¼ í˜ì´ì§€ë„¤ì´ì…˜)"""
    # FDA ì‚¬ì´íŠ¸ì˜ í˜ì´ì§€ë„¤ì´ì…˜ íŒ¨í„´ë“¤
    selectors = [
        # '>' ë²„íŠ¼ (ë‹¤ìŒ í˜ì´ì§€)
        "//a[text()='>']",
        "//a[contains(text(), '>')]",
        # 'Next' í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë§í¬
        "//a[contains(text(), 'Next')]",
        "//a[contains(text(), 'next')]",
        # í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ë“¤ ì¤‘ì—ì„œ í˜„ì¬ë³´ë‹¤ í° ê²ƒ
        "//a[contains(@href, 'PAGENUM=')]",
        # ì¼ë°˜ì ì¸ Next íŒ¨í„´ë“¤
        "//input[@type='submit' and contains(@value, 'Next')]",
        "//input[@type='submit' and contains(@value, 'next')]",
        "//button[contains(text(), 'Next')]",
    ]
    
    # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ íŒŒì•…
    current_page = 1
    try:
        # URLì—ì„œ PAGENUM íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        from urllib.parse import urlparse, parse_qs
        parsed = urlparse(driver.current_url)
        params = parse_qs(parsed.query)
        if "PAGENUM" in params:
            current_pagenum = int(params["PAGENUM"][0])
            current_page = (current_pagenum - 1) // 10 + 1  # PAGENUM 1,11,21... -> í˜ì´ì§€ 1,2,3...
        print(f"    â†’ í˜„ì¬ í˜ì´ì§€: {current_page}")
    except Exception:
        pass
    
    for selector in selectors:
        try:
            elements = driver.find_elements(By.XPATH, selector)
            if elements:
                print(f"    â†’ Next ìš”ì†Œ ë°œê²¬: {selector} ({len(elements)}ê°œ)")
                return elements[0]
        except Exception:
            continue
    
    # íŠ¹ë³„íˆ ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ ì°¾ê¸°
    try:
        next_page_num = current_page + 1
        next_page_links = driver.find_elements(By.XPATH, f"//a[text()='{next_page_num}']")
        if next_page_links:
            print(f"    â†’ ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ ë°œê²¬: {next_page_num}")
            return next_page_links[0]
    except Exception:
        pass
    
    return None


def set_results_per_page_to_500(driver: webdriver.Chrome):
    """Results per Pageë¥¼ 500ìœ¼ë¡œ ì„¤ì •"""
    try:
        # ë” ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ Results per Page ë“œë¡­ë‹¤ìš´ ì°¾ê¸°
        selectors = [
            "//select[option[text()='500']]",
            "//select[option[@value='500']]", 
            "//select[contains(@name, 'page')]",
            "//select[contains(@name, 'results')]",
            "//select[contains(@id, 'page')]",
            "//select[contains(@id, 'results')]",
        ]
        
        select_element = None
        for selector in selectors:
            elements = driver.find_elements(By.XPATH, selector)
            if elements:
                select_element = elements[0]
                print(f"    â†’ Results per Page ë“œë¡­ë‹¤ìš´ ë°œê²¬: {selector}")
                break
        
        if select_element:
            from selenium.webdriver.support.ui import Select
            select = Select(select_element)
            current_value = select.first_selected_option.text
            print(f"    â†’ í˜„ì¬ ì„¤ì •ê°’: {current_value}")
            
            # 500 ì˜µì…˜ ì°¾ê¸°
            options = [option.text for option in select.options]
            print(f"    â†’ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜µì…˜ë“¤: {options}")
            
            if "500" in options and current_value != "500":
                print(f"    â†’ Results per Pageë¥¼ {current_value}ì—ì„œ 500ìœ¼ë¡œ ë³€ê²½")
                select.select_by_visible_text("500")
                time.sleep(3.0)  # í˜ì´ì§€ ë¦¬ë¡œë“œ ëŒ€ê¸°
                WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, "//table")))
                return True
            else:
                print(f"    â†’ Results per Page ì´ë¯¸ 500ìœ¼ë¡œ ì„¤ì •ë¨ ë˜ëŠ” 500 ì˜µì…˜ ì—†ìŒ")
        else:
            print(f"    â†’ Results per Page ë“œë¡­ë‹¤ìš´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            # í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ select íƒœê·¸ ëª¨ë‘ í™•ì¸
            all_selects = driver.find_elements(By.TAG_NAME, "select")
            print(f"    â†’ í˜ì´ì§€ì˜ ì „ì²´ select íƒœê·¸ ê°œìˆ˜: {len(all_selects)}")
            for i, sel in enumerate(all_selects[:3]):  # ì²˜ìŒ 3ê°œë§Œ
                try:
                    select_obj = Select(sel)
                    options = [opt.text for opt in select_obj.options]
                    print(f"      - Select {i+1}: {options}")
                except:
                    pass
                    
    except Exception as e:
        print(f"    â†’ Results per Page ì„¤ì • ì‹¤íŒ¨: {e}")
    return False


def try_next_page(driver: webdriver.Chrome) -> bool:
    """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì‹œë„ (FDA í˜ì´ì§€ë„¤ì´ì…˜ ë°©ì‹)"""
    print(f"ğŸ“ í˜„ì¬ í˜ì´ì§€ ì •ë³´:")
    debug_current_page_info(driver)
    
    before_sig = first_row_signature(driver)
    before_url = driver.current_url
    
    # 1) '>' ë²„íŠ¼ ìš°ì„  ì‹œë„
    next_arrows = driver.find_elements(By.XPATH, "//a[text()='>']")
    if next_arrows:
        try:
            next_elem = next_arrows[0]
            href = next_elem.get_attribute("href")
            print(f"    â†’ '>' ë²„íŠ¼ ë°œê²¬, href: {href}")
            
            if href and href != "javascript:void(0)":
                print(f"    â†’ '>' ë§í¬ë¡œ ì§ì ‘ ì´ë™: {href}")
                driver.get(href)
            else:
                print(f"    â†’ '>' ë²„íŠ¼ í´ë¦­")
                driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(0.3)
                next_elem.click()
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, "//table")))
            time.sleep(2.0)
            
            after_sig = first_row_signature(driver)
            after_url = driver.current_url
            
            print(f"    â†’ ì´ë™ í›„ URL: {after_url}")
            print(f"    â†’ í˜ì´ì§€ ë³€ê²½ë¨: {after_sig != before_sig}")
            
            if after_sig != before_sig:
                result_count = count_total_results(driver)
                print(f"    âœ… ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì„±ê³µ! ê²°ê³¼: {result_count}ê°œ")
                return result_count > 0
            else:
                print(f"    âŒ ê°™ì€ í˜ì´ì§€ (ì‹œê·¸ë‹ˆì²˜ ë™ì¼)")
                
        except Exception as e:
            print(f"    â†’ '>' ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")
    
    # 2) í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ë¡œ ì‹œë„
    try:
        # í˜„ì¬ PAGENUM íŒŒì•…
        from urllib.parse import urlparse, parse_qs
        parsed = urlparse(before_url)
        params = parse_qs(parsed.query)
        current_pagenum = int(params.get("PAGENUM", ["1"])[0])
        next_pagenum = current_pagenum + 10
        
        print(f"    â†’ í˜„ì¬ PAGENUM: {current_pagenum}, ë‹¤ìŒ: {next_pagenum}")
        
        # ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ ì°¾ê¸°
        next_page_links = driver.find_elements(By.XPATH, f"//a[contains(@href, 'PAGENUM={next_pagenum}')]")
        if next_page_links:
            next_elem = next_page_links[0]
            href = next_elem.get_attribute("href")
            print(f"    â†’ ë‹¤ìŒ í˜ì´ì§€ ë§í¬ ë°œê²¬: {href}")
            
            driver.get(href)
            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, "//table")))
            time.sleep(2.0)
            
            after_sig = first_row_signature(driver)
            if after_sig != before_sig:
                result_count = count_total_results(driver)
                print(f"    âœ… í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ë¡œ ì´ë™ ì„±ê³µ! ê²°ê³¼: {result_count}ê°œ")
                return result_count > 0
                
    except Exception as e:
        print(f"    â†’ í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ ì‹œë„ ì‹¤íŒ¨: {e}")
    
    # 3) URL ì§ì ‘ êµ¬ì„±í•´ì„œ ì‹œë„
    try:
        from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
        parsed = urlparse(before_url)
        params = parse_qs(parsed.query)
        
        current_pagenum = int(params.get("PAGENUM", ["1"])[0])
        next_pagenum = current_pagenum + 10
        params["PAGENUM"] = [str(next_pagenum)]
        
        new_query = urlencode({k: v[0] for k, v in params.items()})
        new_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, new_query, parsed.fragment))
        
        print(f"    â†’ URL ì§ì ‘ êµ¬ì„±: {new_url}")
        driver.get(new_url)
        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, "//table")))
        time.sleep(2.0)
        
        after_sig = first_row_signature(driver)
        if after_sig != before_sig:
            result_count = count_total_results(driver)
            print(f"    âœ… URL ì§ì ‘ êµ¬ì„±ìœ¼ë¡œ ì´ë™ ì„±ê³µ! ê²°ê³¼: {result_count}ê°œ")
            return result_count > 0
        else:
            print(f"    âŒ URL ì§ì ‘ êµ¬ì„±ë„ ê°™ì€ í˜ì´ì§€")
            
    except Exception as e:
        print(f"    â†’ URL ì§ì ‘ êµ¬ì„± ì‹¤íŒ¨: {e}")
    
    print(f"    âŒ ëª¨ë“  ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ë°©ë²• ì‹¤íŒ¨")
    return False


def collect_page_rows(driver: webdriver.Chrome) -> List[Dict]:
    """í˜„ì¬ ê²°ê³¼ í˜ì´ì§€ì—ì„œ (k_number, device_name, applicant, decision_date, detail_link) ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    items = []
    rows = safe_find_all_results_rows(driver)
    for row in rows:
        try:
            k_a = row.find_element(By.XPATH, ".//td[1]/a")
            k_number = k_a.text.strip()
            detail_link = k_a.get_attribute("href")

            device_name = row.find_element(By.XPATH, ".//td[2]").get_text() if hasattr(
                row.find_element(By.XPATH, ".//td[2]"), "get_text"
            ) else row.find_element(By.XPATH, ".//td[2]").text
            device_name = device_name.strip()

            applicant = row.find_element(By.XPATH, ".//td[3]").text.strip()
            decision_date = row.find_element(By.XPATH, ".//td[4]").text.strip()

            # ì¼ë¶€ í™˜ê²½ì—ì„œ ì²« ì—´ í…ìŠ¤íŠ¸ê°€ K-number ëŒ€ì‹  ê¸°ê¸°ëª…ì¸ ê²½ìš°ê°€ ìˆì–´ ìƒì„¸ì—ì„œ êµì •ë¨
            # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, ê³µë°± ì œê±°
            k_number = k_number.strip()

            items.append(
                {
                    "k_number": k_number,
                    "device_name": device_name,
                    "applicant": applicant,
                    "decision_date": decision_date,
                    "detail_link": detail_link,
                }
            )
        except Exception as e:
            print(f"    â†’ í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
            continue
    return items


def find_best_pdf_link_on_detail(driver: webdriver.Chrome) -> Tuple[Optional[str], str]:
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ìµœì ì˜ PDF ë§í¬ ì¶”ì¶œ.
    ë°˜í™˜: (pdf_url, pdf_type)  # pdf_type in {"summary","backup","any",""}
    ìš°ì„ ìˆœìœ„:
      1) í…ìŠ¤íŠ¸/URLì— 'summary' í¬í•¨ë˜ëŠ” PDF
      2) BACKUP_PDF_KEYWORDSì— í•´ë‹¹í•˜ëŠ” PDF (e.g., Decision Letter)
      3) í˜ì´ì§€ ë‚´ ì„ì˜ì˜ PDF
    """
    anchors = driver.find_elements(By.XPATH, "//a[@href]")
    cand_summary = []
    cand_backup = []
    cand_any_pdf = []

    for a in anchors:
        try:
            txt = (a.text or "").strip()
            href = a.get_attribute("href") or ""
            if not href:
                continue
            tl = txt.lower()
            hl = href.lower()

            is_pdfish = hl.endswith(".pdf") or "pdf" in hl
            if not is_pdfish:
                continue

            # 1) summary ê³„ì—´
            if any(k in tl for k in SUMMARY_KEYWORDS) or any(k in hl for k in SUMMARY_KEYWORDS):
                cand_summary.append(href)
                continue

            # 2) backup í‚¤ì›Œë“œ (decision letter ë“±)
            if any(k in tl for k in BACKUP_PDF_KEYWORDS) or any(k in hl for k in BACKUP_PDF_KEYWORDS):
                cand_backup.append(href)
                continue

            # 3) ê·¸ ì™¸ ëª¨ë“  pdf
            cand_any_pdf.append(href)

        except StaleElementReferenceException:
            continue

    if cand_summary:
        return cand_summary[0], "summary"
    if cand_backup:
        return cand_backup[0], "backup"
    if cand_any_pdf:
        return cand_any_pdf[0], "any"
    return None, ""


def open_in_new_tab_and_process(
    driver: webdriver.Chrome, url: str, process_fn
) -> Optional[Dict]:
    """
    í˜„ì¬ íƒ­ì€ ìœ ì§€í•˜ê³  ìƒˆ íƒ­ìœ¼ë¡œ ì—´ì–´ process_fn ì‹¤í–‰ í›„ ë‹«ê³  ì›ë˜ íƒ­ìœ¼ë¡œ ë³µê·€
    """
    main_handle = driver.current_window_handle
    driver.execute_script("window.open(arguments[0], '_blank');", url)

    WebDriverWait(driver, 10).until(EC.number_of_windows_to_be(2))
    handles = driver.window_handles
    new_handle = [h for h in handles if h != main_handle][0]
    driver.switch_to.window(new_handle)
    try:
        result = process_fn()
    finally:
        # ìƒˆ íƒ­ ë‹«ê³  ë³µê·€
        driver.close()
        driver.switch_to.window(main_handle)
    return result


def infer_k_number_from_page(driver) -> Optional[str]:
    """ìƒì„¸ í˜ì´ì§€ ë³¸ë¬¸ì—ì„œ K-number (ì˜ˆ: K123456) ì¶”ì¶œ"""
    try:
        body_txt = driver.find_element(By.TAG_NAME, "body").get_attribute("innerText")
        m = re.search(r"\bK\d{6}\b", body_txt)
        return m.group(0) if m else None
    except Exception:
        return None


def process_detail_page(driver: webdriver.Chrome, it: Dict) -> Dict:
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ Summary/Letter PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
    """
    sess = requests_session_from_driver(driver)
    rand_sleep(0.6, 1.2)

    pdf_url, pdf_type = find_best_pdf_link_on_detail(driver)
    summary_link = None
    summary_text = None
    if pdf_url:
        summary_link = urljoin(driver.current_url, pdf_url)
        summary_text = extract_pdf_text_with_session(sess, summary_link, referer=driver.current_url)
    else:
        print(f"[PDF ë§í¬ ì—†ìŒ] {it['k_number']}")

    # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì§„ì§œ K-numberë¥¼ ì°¾ì•„ ë®ì–´ì“°ê¸°
    knum = infer_k_number_from_page(driver)
    if knum:
        it = {**it, "k_number": knum}

    return {**it, "summary_link": summary_link, "summary_text": summary_text, "pdf_type": pdf_type}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Core crawl
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawl_fda_510k(
    query: str,
    max_pages: int,
    out_path: str,
    headless: bool = True,  # ê¸°ë³¸ê°’ì„ Trueë¡œ ë³€ê²½
    throttle: float = 0.8,
    resume: bool = True,
) -> None:
    """
    queryë¡œ ê²€ìƒ‰ â†’ ê²°ê³¼ ìˆœíšŒ â†’ ìƒì„¸ì˜ Summary PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ JSONL ì €ì¥
    """
    # ì´ë¯¸ ì €ì¥ëœ k_number ìŠ¤í‚µ(ì˜µì…˜)
    seen = set()
    if resume:
        try:
            with open(out_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        obj = json.loads(line)
                        if "k_number" in obj:
                            seen.add(obj["k_number"])
                    except Exception:
                        continue
            print(f"[resume] {len(seen)}ê°œ ìŠ¤í‚µ ì˜ˆì •")
        except FileNotFoundError:
            pass

    driver = setup_driver(headless=headless)
    total_written = 0
    consecutive_empty_pages = 0  # ì—°ì† ë¹ˆ í˜ì´ì§€ ì¹´ìš´í„°
    
    try:
        driver.get(BASE_URL)
        wait_for_search_box(driver)

        # ê²€ìƒ‰ì–´ ì…ë ¥
        box = driver.find_element(By.CSS_SELECTOR, "input[name='DeviceName']")
        box.clear()
        box.send_keys(query)
        driver.find_element(By.XPATH, "//input[@value='Search']").click()
        
        # ì²« í˜ì´ì§€ ë¡œë“œ í™•ì¸
        rand_sleep(2.0, 3.0)
        
        # Results per Pageë¥¼ 500ìœ¼ë¡œ ì„¤ì •
        set_results_per_page_to_500(driver)
        
        page = 1
        with open(out_path, "a", encoding="utf-8") as out_f:
            while True:
                print(f"ğŸ“„ '{query}' / page={page}")
                
                items = collect_page_rows(driver)
                print(f"  - rows: {len(items)}")
                
                # ë¹ˆ í˜ì´ì§€ ì²˜ë¦¬
                if len(items) == 0:
                    consecutive_empty_pages += 1
                    print(f"  - ë¹ˆ í˜ì´ì§€ (ì—°ì† {consecutive_empty_pages}íšŒ)")
                    if consecutive_empty_pages >= 3:  # ì—°ì† 3íšŒ ë¹ˆ í˜ì´ì§€ë©´ ì¢…ë£Œ
                        print("  - ì—°ì† ë¹ˆ í˜ì´ì§€ í•œê³„ë¡œ ì¢…ë£Œ")
                        break
                    # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì‹œë„
                    if try_next_page(driver):
                        page += 1
                        continue
                    else:
                        print("  - ë” ì´ìƒ ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ì–´ í¬ë¡¤ë§ ì¢…ë£Œ")
                        break
                else:
                    consecutive_empty_pages = 0  # ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¹´ìš´í„° ë¦¬ì…‹

                # ìƒì„¸ ë§í¬ë“¤ì„ ë¯¸ë¦¬ ë¦¬ìŠ¤íŠ¸ ë³µì‚¬(íƒ­ ì—´ì–´ ë‹«ê¸° ì „ ì›ë³¸ DOM ì˜ì¡´ X)
                detail_targets = [(it["k_number"], it["detail_link"], it) for it in items]

                page_written = 0  # í˜„ì¬ í˜ì´ì§€ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ í•­ëª© ìˆ˜
                
                for k_number, detail_url, it in detail_targets:
                    if resume and k_number in seen:
                        # ì´ë¯¸ ì €ì¥ëë˜ í•­ëª©ì€ ê±´ë„ˆë›°ê¸°
                        print(f"  â€¢ {k_number} (skip: resume)")
                        continue

                    def _process():
                        # ìƒì„¸ íƒ­ì—ì„œ ì²˜ë¦¬
                        WebDriverWait(driver, 15).until(
                            EC.presence_of_element_located((By.TAG_NAME, "body"))
                        )
                        enriched = process_detail_page(driver, it)
                        return enriched

                    try:
                        enriched = open_in_new_tab_and_process(driver, detail_url, _process)
                        if enriched is None:
                            continue
                            
                    except (TimeoutException, WebDriverException) as e:
                        print(f"[ìƒì„¸ ì²˜ë¦¬ ì‹¤íŒ¨] {k_number}: {e}")
                        continue

                    out_f.write(json.dumps(enriched, ensure_ascii=False) + "\n")
                    out_f.flush()
                    total_written += 1
                    page_written += 1

                    # ì§„í–‰ ë¡œê·¸
                    st_len = len(enriched["summary_text"]) if enriched.get("summary_text") else 0
                    print(f"  â€¢ {k_number}: text_len={st_len}")

                    # ì„œë²„ ë°°ë ¤
                    rand_sleep(throttle, throttle + 0.9)

                print(f"  - í˜ì´ì§€ {page} ì™„ë£Œ: {page_written}ê°œ ì²˜ë¦¬ë¨")

                if max_pages > 0 and page >= max_pages:
                    print(f"  - ìµœëŒ€ í˜ì´ì§€ ìˆ˜({max_pages}) ë„ë‹¬ë¡œ ì¢…ë£Œ")
                    break

                page += 1
                rand_sleep(1.8, 2.8)

        print(f"âœ… ì™„ë£Œ: {total_written}ê°œ ì €ì¥ â†’ {out_path}")

    finally:
        try:
            driver.quit()
        except Exception:
            pass


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser(description="FDA 510(k) Summary PDF text crawler")
    parser.add_argument("--query", type=str, default="implant", help="DeviceName ê²€ìƒ‰ì–´")
    parser.add_argument("--max-pages", type=int, default=50, help="ìµœëŒ€ í˜ì´ì§€ ìˆ˜(10ê°œì”©). 0=ëê¹Œì§€")
    parser.add_argument("--out", type=str, default=None, help="ì €ì¥ ê²½ë¡œ(JSONL)")
    parser.add_argument("--headless", action="store_true", help="Headless ëª¨ë“œ")
    parser.add_argument("--no-resume", action="store_true", help="ê¸°ì¡´ JSONL ë¬´ì‹œí•˜ê³  ì²˜ìŒë¶€í„°")
    parser.add_argument("--throttle", type=float, default=0.8, help="ìš”ì²­ ê°„ ìµœì†Œ ëŒ€ê¸°(ì´ˆ)")

    args = parser.parse_args()
    out_path = args.out or f"fda_{args.query.replace(' ', '_')}.jsonl"

    crawl_fda_510k(
        query=args.query,
        max_pages=args.max_pages,
        out_path=out_path,
        headless=args.headless,
        throttle=args.throttle,
        resume=not args.no_resume,
    )


if __name__ == "__main__":
    main()